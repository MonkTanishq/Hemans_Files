<!DOCTYPE html>
<html lang="en">
  <head>
    <title>HemansAI</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="Artificial Intelligence research based in Indore" />
    <meta name="keywords" content="python,learn,machine learning,web development,artificial intelligence,deep learning,data science,indore,AI,industrial training,internship,in" />
	<link rel="icon" type="image/ico" href="images/logo2.png" />
    
    <link href="https://fonts.googleapis.com/css?family=Raleway" rel="stylesheet">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/open-iconic-bootstrap.min.css">
    <link rel="stylesheet" href="css/animate.css">
    
    <link rel="stylesheet" href="css/owl.carousel.min.css">
    <link rel="stylesheet" href="css/owl.theme.default.min.css">

    <link rel="stylesheet" href="css/icomoon.css">
    <link rel="stylesheet" href="css/style.css">
  </head>
  <body data-spy="scroll" data-target="#ftco-navbar" data-offset="200">
    <nav class="navbar navbar-expand-lg navbar-dark ftco_navbar bg-dark ftco-navbar-light" id="ftco-navbar">
      <div class="container">
        <a class="navbar-brand" href="index.html">Hemans AI</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#ftco-nav" aria-controls="ftco-nav" aria-expanded="false" aria-label="Toggle navigation">
          <span class="oi oi-menu"></span> Menu
        </button>

        <div class="collapse navbar-collapse" id="ftco-nav">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item active"><a href="#section-home" class="nav-link">Home</a></li>
            <li class="nav-item"><a href="#section-features" class="nav-link">Deep Learning</a></li>
            <li class="nav-item"><a href="#section-contact" class="nav-link">Contact</a></li>
          </ul>
        </div>
      </div>
    </nav>
    <!-- END nav -->

    <section class="ftco-cover ftco-slant" id="section-home" style="background-image:url('images/dlimg1.jpeg');">
      <div class="container">
        <div class="row align-items-center justify-content-center text-center ftco-vh-100">
          <div class="col-md-10" style="margin-top:-60px;">
            
		  </div>
        </div>
      </div>
    </section>
    
    

    <section class="ftco-section bg-light  ftco-slant ftco-slant-white" id="section-features">
      <div class="container">
        
        <div class="row">
          <div class="col-md-12 text-center mb-5 ftco-animate">
            <h2 class="text-uppercase ftco-uppercase">Deep Learning<br><br><br><br></h2>
            <div class="row justify-content-center">
              <div class="col-md-7">
			  
                <p class="lead">Deep Learning is a subfield of machine learning concerned 
				with algorithms inspired by the structure and function of the brain called 
				artificial neural networks.<br>

If you are just starting out in the field of deep learning or you had some experience with 
neural networks some time ago, you may be confused. I know I was confused initially and so 
were many of my colleagues and friends who learned and used neural networks in the 1990s 
and early 2000s.<br>
In this post, you will discover exactly what deep learning is by hearing from a range 
of experts and leaders in the field.</p><br>
				
				
              </div>
            </div>
          </div>
		  <p style="font-size:18px; color:black;">
		  <span style="font-size:25px;">Deep Learning is Large Neural Networks</span>
<br><br>
Andrew Ng from Coursera and Chief Scientist at Baidu Research formally founded Google Brain 
that eventually resulted in the productization of deep learning technologies across a large 
number of Google services.<br>

He has spoken and written a lot about what deep learning is and is a good place to start.<br>

In early talks on deep learning, Andrew described deep learning in the context of 
traditional artificial neural networks. In the 2013 talk titled “Deep Learning, 
Self-Taught Learning and Unsupervised Feature Learning” he described the idea of deep 
learning as:<br><br>

    Using brain simulations, hope to:<br>

    – Make learning algorithms much better and easier to use.<br>
    – Make revolutionary advances in machine learning and AI.<br>

I believe this is our best shot at progress towards real AI<br>

Later his comments became more nuanced.<br>
<br>
The core of deep learning according to Andrew is that we now have fast enough computers 
and enough data to actually train large neural networks. When discussing why now is the 
time that deep learning is taking off at ExtractConf 2015 in a talk titled “What data 
scientists should know about deep learning“, he commented:<br><br>

    "Very large neural networks we can now have and … huge amounts of data that we have 
	access to.."<br><br>

He also commented on the important point that it is all about scale. 
That as we construct larger neural networks and train them with more and more data, 
their performance continues to increase. This is generally different to other machine 
learning techniques that reach a plateau in performance.<br><br>

    for most flavors of the old generations of learning algorithms … performance will 
	plateau. … deep learning … is the first class of algorithms … that is scalable. … 
	performance just keeps getting better as you feed them more data<br><br>


Finally, he is clear to point out that the benefits from deep learning that we are 
seeing in practice come from supervised learning. From the 2015 ExtractConf talk, 
he commented:<br><br>

    "Almost all the value today of deep learning is through supervised learning or learning 
	from labeled data"<br><br>

Earlier at a talk to Stanford University titled “Deep Learning” in 2014 he made a similar 
comment:<br><br>

    One reason that deep learning has taken off like crazy is because it is fantastic at 
	supervised learning"<br><br>

Andrew often mentions that we should and will see more benefits coming from the 
unsupervised side of the tracks as the field matures to deal with the abundance of 
unlabeled data available.<br>

Jeff Dean is a Wizard and Google Senior Fellow in the Systems and Infrastructure 
Group at Google and has been involved and perhaps partially responsible for the 
scaling and adoption of deep learning within Google. Jeff was involved in the Google 
Brain project and the development of large-scale deep learning software DistBelief and 
later TensorFlow.<br><br>

In a 2016 talk titled “Deep Learning for Building Intelligent Computer Systems” 
he made a comment in the similar vein, that deep learning is really all about large 
neural networks.<br>

    When you hear the term deep learning, just think of a large deep neural net. 
	Deep refers to the number of layers typically and so this kind of the popular 
	term that’s been adopted in the press. I think of them as deep neural networks 
	generally.<br>

He has given this talk a few times, and in a modified set of slides for the same talk, 
he highlights the scalability of neural networks indicating that results get better with 
more data and larger models, that in turn require more computation to train.<br><br>
<br>
<br>

<span style="font-size:25px;">Deep Learning is Hierarchical Feature Learning</span>
<br>
<br>

In addition to scalability, another often cited benefit of deep learning models is their 
ability to perform automatic feature extraction from raw data, also called feature learning.<br>

Yoshua Bengio is another leader in deep learning although began with a strong interest 
in the automatic feature learning that large neural networks are capable of achieving.<br>

He describes deep learning in terms of the algorithms ability to discover and learn good 
representations using feature learning. In his 2012 paper titled “Deep Learning of 
Representations for Unsupervised and Transfer Learning” he commented:<br><br>

    "Deep learning algorithms seek to exploit the unknown structure in the input distribution 
	in order to discover good representations, often at multiple levels, with higher-level 
	learned features defined in terms of lower-level features"<br><br>

An elaborated perspective of deep learning along these lines is provided in his 2009 
technical report titled “Learning deep architectures for AI” where he emphasizes the 
importance the hierarchy in feature learning.<br><br>

    Deep learning methods aim at learning feature hierarchies with features from higher 
	levels of the hierarchy formed by the composition of lower level features. 
	Automatically learning features at multiple levels of abstraction allow a system 
	to learn complex functions mapping the input to the output directly from data, without 
	depending completely on human-crafted features.<br><br>

In the soon to be published book titled “Deep Learning” co-authored with Ian Goodfellow 
and Aaron Courville, they define deep learning in terms of the depth of the architecture 
of the models.<br>

    The hierarchy of concepts allows the computer to learn complicated concepts by 
	building them out of simpler ones. If we draw a graph showing how these concepts 
	are built on top of each other, the graph is deep, with many layers. For this reason, 
	we call this approach to AI deep learning.<br><br>

This is an important book and will likely become the definitive resource for the field for 
some time. The book goes on to describe multilayer perceptrons as an algorithm used in the 
field of deep learning, giving the idea that deep learning has subsumed artificial neural 
networks.<br>

    The quintessential example of a deep learning model is the feedforward deep network 
	or multilayer perceptron (MLP).<br><br>

Peter Norvig is the Director of Research at Google and famous for his textbook on AI 
titled “Artificial Intelligence: A Modern Approach“.<br>

In a 2016 talk he gave titled “Deep Learning and Understandability versus Software 
Engineering and Verification” he defined deep learning in a very similar way to Yoshua, 
focusing on the power of abstraction permitted by using a deeper network structure.<br>

    a kind of learning where the representation you form have several levels of abstraction, 
	rather than a direct input to output<br><br>

<span style="font-size:25px;">Why Call it “Deep Learning”?<br>
Why Not Just “Artificial Neural Networks”?</span>
<br><br>
Geoffrey Hinton is a pioneer in the field of artificial neural networks and co-published 
the first paper on the backpropagation algorithm for training multilayer perceptron networks.
<br>
He may have started the introduction of the phrasing “deep” to describe the 
development of large artificial neural networks.<br>

He co-authored a paper in 2006 titled “A Fast Learning Algorithm for Deep Belief Nets” 
in which they describe an approach to training “deep” (as in a many layered network) 
of restricted Boltzmann machines.<br><br>

    Using complementary priors, we derive a fast, greedy algorithm that can learn deep, d
	irected belief networks one layer at a time, provided the top two layers form an undirected 
	associative memory.<br>

This paper and the related paper Geoff co-authored titled “Deep Boltzmann Machines” 
on an undirected deep network were well received by the community (now cited many hundreds 
of times) because they were successful examples of greedy layer-wise training of networks, 
allowing many more layers in feedforward networks.<br><br>

In a co-authored article in Science titled “Reducing the Dimensionality of Data with Neural 
Networks” they stuck with the same description of “deep” to describe their approach to 
developing networks with many more layers than was previously typical.<br>

    We describe an effective way of initializing the weights that allows deep
	autoencoder networks to learn low-dimensional codes that work much better than principal 
	components analysis as a tool to reduce the dimensionality of data.<br>

In the same article, they make an interesting comment that meshes with Andrew Ng’s 
comment about the recent increase in compute power and access to large datasets that 
has unleashed the untapped capability of neural networks when used at larger scale.<br><br>

    It has been obvious since the 1980s that backpropagation through deep autoencoders 
	would be very effective for nonlinear dimensionality reduction, provided that computers
	were fast enough, data sets were big enough, and the initial weights were close enough 
	to a good solution. All three conditions are now satisfied.<br><br>

In a talk to the Royal Society in 2016 titled “Deep Learning“, Geoff commented that Deep 
Belief Networks were the start of deep learning in 2006 and that the first successful 
application of this new wave of deep learning was to speech recognition in 2009 titled 
“Acoustic Modeling using Deep Belief Networks“, achieving state of the art results.<br>

It was the results that made the speech recognition and the neural network communities 
take notice, the use “deep” as a differentiator on previous neural network techniques 
that probably resulted in the name change.<br>

The descriptions of deep learning in the Royal Society talk are very backpropagation 
centric as you would expect. Interesting, he gives 4 reasons why backpropagation 
(read “deep learning”) did not take off last time around in the 1990s. The first two 
points match comments by Andrew Ng above about datasets being too small and computers 
being too slow.<br><br>

<span style="font-size:25px;">Summary</span>
<br><br>
In this post you discovered that deep learning is just very big neural networks on a 
lot more data, requiring bigger computers.<br>

Although early approaches published by Hinton and collaborators focus on 
greedy layerwise training and unsupervised methods like autoencoders, modern 
state-of-the-art deep learning is focused on training deep (many layered) neural 
network models using the backpropagation algorithm. The most popular techniques are:<br><br>

    Multilayer Perceptron Networks.<br>
    Convolutional Neural Networks.<br>
    Long Short-Term Memory Recurrent Neural Networks.<br><br>

I hope this has cleared up what deep learning is and how leading definitions fit
together under the one umbrella.<br>

<p class="lead" style="font-size:30px;">Want to learn Deep Learning in Indore. 
Feel free to <br><a href="#section-contact">Contact Us</a></p>

</p>
        </div>
      </div>
    </section>
    <!-- END section -->

    
    <section class="ftco-section bg-white ftco-slant ftco-slant-dark" id="section-contact">
      <div class="container">
	  <center>
        <div class="row">

          <div class="col-md pr-md-5 mb-5">
		    <h1><b>Let's Learn Deep Learning with Python<span><b></h1><br>
            <a class="btn btn-primary btn-xlg" href="tel: +9190749191879" style="padding:20px;">
			Click to Call <span class="glyphicon glyphicon-earphone" style="padding-left:10px; padding-right:10px;"></span>+91 9074919189
			</a>
          </div>
          
        </div>
	  </center>
      </div>
    </section>
    <footer class="ftco-footer ftco-bg-dark">
      <div class="container">
        <div class="row mb-5">
          <div class="col-md-8">
            <div class="row">
              <div class="col-md">
                <div class="ftco-footer-widget mb-4">
                  <h2 class="ftco-heading-2">Company</h2>
                  <ul class="list-unstyled">
                   <li><a href="index.html#section-about" class="py-2 d-block">About</a></li>
                    <li><a href="index.html#section-features" class="py-2 d-block">Jobs</a></li>
                    <li><a href="index.html#section-pricing" class="py-2 d-block">Press</a></li>
                    <li><a href="index.html#section-services" class="py-2 d-block">News</a></li>
                  </ul>
                </div>
              </div>
             
            
            </div>
          </div>
          <div class="col-md-4">
            <div class="ftco-footer-widget mb-4">
              <ul class="ftco-footer-social list-unstyled float-md-right float-lft">
                <li><a href="https://twitter.com/he_mans_nation"><span class="icon-twitter"></span></a></li>
				<li><a href="https://github.com/hemansnation"><span class="icon-github"></span></a></li>
                <li><a href="https://www.facebook.com/hemansnation"><span class="icon-facebook"></span></a></li>
                <li><a href="https://www.instagram.com/he_mans_nation/"><span class="icon-instagram"></span></a></li>
              </ul>
            </div>
          </div>
        </div>
        <div class="row">
          <div class="col-md text-left">
            <p>&copy; HemansAI 2018. All Rights Reserved.
          </div>
        </div>
      </div>
    </footer>

    <!-- loader -->
    <div id="ftco-loader" class="show fullscreen"><svg class="circular" width="48px" height="48px"><circle class="path-bg" cx="24" cy="24" r="22" fill="none" stroke-width="4" stroke="#eeeeee"/><circle class="path" cx="24" cy="24" r="22" fill="none" stroke-width="4" stroke-miterlimit="10" stroke="#4586ff"/></svg></div>

	<script src="js/particles.js"></script>
	<script src="js/app.js"></script>
    <script src="js/jquery.min.js"></script>
    <script src="js/popper.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/jquery.easing.1.3.js"></script>
    <script src="js/jquery.waypoints.min.js"></script>
    <script src="js/owl.carousel.min.js"></script>
    <script src="js/jquery.animateNumber.min.js"></script>
    

    <script src="https://maps.googleapis.com/maps/api/js?key=AIzaSyBVWaKrjvy3MaE7SQ74_uJiULgl1JY0H2s&sensor=false"></script>
    <script src="js/google-map.js"></script>

    <script src="js/main.js"></script>

    
  </body>
</html>